---
title: "Caladan: Mitigating Interference at Microsecond Timescales"
date: 2021-07-01
categories:
  - Paper Review
tags:
  - OSDI
---

# Intro
- datacenter involves thousands of machines 
- The slowest machine can affect the overall performance, so tail-latency must be kept low
- But, must balance the latency with efficiency (ideal: operate HW at 100% utilization). Existing approaches:
  + Pack multiple tasks on each machine
  + Give higher priority for latency-ciritical tasks (ex. Memcached), and allow the rest of resources to other best-effort tasks
  + Unfortunately, packing many tasks leads to the noisy neighbor problem 

### Noisy neighbor problem
- Tasks contend for the same resources (e.g., cores, caches, memory BW, shared execution units)
- When the contention is severe, apps experience interference
- Latency-critical apps may fail to meet the latency objectives
- These resource contention issues are hard to handle 
- Why? 
  + App resource usage is constantly shifting, so the patterns of interference change over time. Examples:
    * Google and MS reports: their datacenter services regularly experience burst in load that can occur over us-time scales
    * These busrt can cause rapid changes in resource demand and interference
  + Phased behavior is common among datacenter tasks 
    * e.g., compression, compilation, spark compute jobs, garbage collenction
    * it can change the resource demand suddenly at sub-second time scales
  
##### Example case 
- Two tasks (Memcached latency-critical task and best-effort task with Garbage Collection) on a machine
- Give 2 isolated cores to Memcached (enough to handle the offered load with tail-latencies < 50us)
- Give 20 cores to Garbage Collection task (aiming to keep the utilization of machine high)
- GC task shows saturated memory BW sometimes when the GC happens, and right after then, Memcached shows spikes (>1000x) in latency due to severe interference 
- If we want to switch to a different allocation in only the times interference begins, how fast do we have to react?
  + In a closer look, it takes about 100us to reach 100us latency (i.e., we have to react within 100us to prevent this)
  + Targeting lower latency requires even faster reaction
Q. can we allocated a limited amount of memory BW to each task?

##### Existing Solution
- Partitioning HW resources between apps to avoid contention (dedicating cores, partitioning caches)
- Some recent systems dynamically adjust the resource partitions 
  + Heracles: adjust every 2 secs and converges to new config after 30 secs
  + Parties: converges in 10-20 secs
- Too slow for the GC example above (reaction required within 100us)

Q. is it normal that the latency bound requires 100us scale reaction required? sounds like existing solutions (with 10s seconds converge) are useless..

### Caladan
- Goal: provide strict performance isolation and high resource utilization for datacenter servers
- Requirement: detection and mitigation of interference at us-time scales
##### Challenges at the us scale
- Finding signals that accurately indicate interference
  + Multiple types of interferences (LLC, Memory BW, etc.)
  + Many possible tasks could be causing interferences
  + Commonly used signals (e.g., end-to-end tail latencies, CPI) take >ms to stabilize
- Gathering signals and reacting with low overheads
  + Existing mechanisms for gathering signals and reallocating resources don't scale with many cores and tasks

##### Contributions
- Use exclusive core allocation 
  + Prev systems rely on partitioning other HW resources (e.g., caches, Memory BW, etc.). (Why?)
- Introduce new signals for multiple forms of interferences (accurately identifying the type and source in us)
- KSCHED: a new kernel module 
  + to make signal gathering and core allocation scalable
  + can collect perf counters from all cores in several us

##### Components
- A dedicated Scheduler core
  + spin polls for interference signals
  + decide core allocation to tasks
- Tasks link with runtimes
  + provide threading, I/O, etc.
  + expose signals to the scheduler
- The scheduler core uses KSCHED

##### Two forms of interferences
- Memory BW saturation
  + Monitor two signals: DRAM BW, LLC misses
  + Action: revoke cores from antagonist tasks
- Hyper-thread interference
  + Caladan pairs different tasks on hyperthread sibling cores 
  + Monitor request processing time
  + If an individual request processing time exceeds the fixed budget, the scheduler removes the tasks running on the sibling core for the request duration to prevent any further hyperthread interference
- LLC interference
  + Monitor queueing delay
  + Unlike above two, Caladan cannot use core allocation to directly prevent it
  + Instead, it adds core to victim 


##### How to gathre signals?
- Scheduler 
  + pulls its signal every 10us from several sources
  + It uses memory mapped I/O to read counters directly from DRAM controller to determine memory BW
- Runtimes
  + use shared memory to make information about queueing delays and request processing delays easily visible to the scheduler
- KSCHED
  + Trigger sampling of LLC miss counters on remote cores
  + Uses shared memory to read back the sampled values


# Garbage Collection (in JAVA)
### Why JAVA has it?
- C or C++: you manage the memory. 
  + C: malloc() / realloc() / calloc() to allocate some buffer in memory, and then free() 
  + If you keep allocating w/o free(), memory leaks happen
  + C++: *new* (constructors), and destructors
- JAVA (and most of modern languages): automatic memory management via a background thread: Garbage Collector 
  + GC monitors the memory at the background, and removes unreachable (out of scope, dead objects) allocated memories  
  + Hypothesis: 1) most objects soon become unreachable; 2) references from 'old' objects to 'young' objects only exist in small numbers 

### Basics 
- Objects are allocated (e.g., *new*, arrays) in the heap of JAVA memory
- Static members and Class definitions (metadata) are stored in method aread (Permgen/Metaspace)
- Garbage Collector is a daemon thread
- There is a static method call API *System.gc()*, but we cannot force GC to happen
- *java.lang.OutOfMemoryError*: a runtime error that JVM cannot allocate new object in the heap due to limited space (need to inspect any object or memory leaks)
- GC does not remove *java.lang.OutOfMemoryError* completely


### 3 Steps in GC
1) Mark
  - GC walks through the object graph from the root node (main)
  - marks reachable objects as live
2) Delete/Sweep
  






  
  
  
  
  
  
  




NUMA is a system that is used in computers with multiple physical CPUs.

# Typical Computer
- One CPU handling most of the computational stuff
- RAM is direcntly connected to the CPU. 
- RAM is not a storage memory, but a working memory of the CPU: CPU can temporarily store things that it's working on 
- RAM's capacity is fairly small (a few GB), but it needs to be incredibly fast (to support incredibly fast CPU's computation)
- BUS is the wires connecting between RAM and CPU (Obviously, BUS is very fast to support the communication between RAM and CPU)

# Computer with 4 CPUs
### How to connect those CPUs to the memory?
- Traditionally, connected all CPUs to a big RAM through one BUS (as if there is one massive CPU)
- Bad idea. Why? Obviouslty, the single BUS is easily saturated
- Here, **NUMA** comes in!

# NUMA: a pair of CPU and RAM connected by a BUS
- On multi-CPU computer, each CPU form a NUMA node, and we simply connect them
- Now, all CPUs can access all momories
- But, accessing it's own memory is faster than accessing another CPU's memory (i.e., Non Uniform)
- Nevertheless, accessing other's RAM is faster than the single BUS saturated by multiple CPUs


##### References
[1] https://www.youtube.com/watch?v=G-v3ndwixOI
{"mode":"full","isActive":false}
