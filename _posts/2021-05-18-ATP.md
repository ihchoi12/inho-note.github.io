---
title: "ATP: In-network Aggregation for Multi-tenant Learning"
date: 2021-05-13
categories:
  - Paper Review
tags:
  - ATP
---

# Talk Summary
- Goal: to accelerate the distributed DNN training with Parameter Server (PS) Architecture
### How Distributed Training (DT) with PS Works?
- Two components: 1) workers; 2) PS.
- Each Worker generate a set of gradients by GPU computation
- The set of gradients is splitted into a set of packets
- Each worker transimit it's first packet to PS
- PS aggregate the first packets across all the workers, and then do some complex computation (e.g., optimization)
- Then, PS multicast the results (called parameters) back to all the workers
- These are repeated until all the gradient packets are aggregated up

**Observation: the last link to the PS can be the network bottleneck**

### Idea: Programmable siwtch can reduce this bottleneck?
- Two capabilities: 1) programmable data-plane; 2) state maintaining across packets (i.e., register).
- Goal: reduce training time by moving gradient aggregation into the network.

### State-of-the-art: SwitchML (NSDI'21)
- Target single-rack setting
- To support multiple jobs, statically partitions the switch memory among the jobs through their entire running
- Shortcomings: DT jobs go through *on & off* aggregation phases
- Why *off* happens? Throughput becomes zero when the computation takes over (i.e., no network communication happens, but the switch memroy is still reserved for the jobs)

**Observation: 1) the static allocation can lead to the inefficient usage of the switch resouces; 2) a single job in larger ML models can span multiple racks;**

# ATP: to speed up multiple DT jobs in a cluster while maximizing the benefits from in-network multi-switch aggregation

### Multi-tenant: Dynamic Allocation
- ATP chooses to dynamically allocate the switch memory in per-packet level
- Why? to support multiple jobs simultanenously while maximizing switch resource utilization
- How? Randomly hash gradient packets to whole memory 

### How ATP Works?
- 1) workers hash gradient index to identify an aggregator at switch; 
- 2) (if its available) switch stores those gradient pkts and aggregates the incoming gradients from all workers
- 3) switch sends the aggregation result to the PS
- 4) PS sends the parameter pkt back to the switch
- 5) switch frees the aggregator and multicast the results to all the workers 

### Challenge1: what if switch memory is saturated?
- in the heavy contention, the aggregators may not be available. 
- ATP's *best-effort* approach: a fraction of aggregation will fall back to the end-hosts
- 1) a worker's gradient pkt arrives at an aggregator occupied by another job already
- 2) the packet and other workers' pkts are just forwarded to the PS 
- 3) PS does the aggregation over these gradient pkts from all workers, and send the parameter back to the switch
- 4) Switch multicast it back to the workers

### Challenge2: incomplete aggregation
- First gradieint packets from some workers were forwarded to the PS directly (due to inavailability), but the others are aggregated in the switch
- Either PS or the switch cannot complete the aggregation for the first gradient
- Eventually, the workers will receive the parameters for later gradients
- Then, workers retry the aggregation of the first gradient 

# Inter-Rack Aggregation
- Workers and PS(es) can be located in different racks for large-scale DT jobs, which requires inter-rack aggregation
- Aggregation can be done at every later of the network topology 
- However, aggregation at higher layers can increase the protocol complexity due to ECMP-like non-deterministic routing 
- To minimize network changes, ATP supports two-level aggregation at ToR switches
- 1) aggregation of gradient pkts is done at the local ToR switch first
- 2) the local ToR switches send the partial aggregation result to the PS's ToR switch
- 3) PS's ToR switch does the second level of aggregation over those partial aggregation results 
- By doing so, ATP can scale up to 1024 workers

# Additional Challenges
### Reliablity
- usually means recovery from packet loss, but here ensuring exact once aggregation too
- In addition, ATP does not have the controller to de-allocate the aggregators for a job (i.e., memory leak -- aggregators reserved but not used -- possible)
### Congestion Control
- ATP is designed to be co-located with other jobs in a cluster
- Rethink with the changed communication pattern (N flows are merged into one flow)
- Congestion singal (e.g., ECN) might be dropped
### Floating Point Computation
- Currently, programmable switches do not support it 
- Similarly to SwitchML, ATP converts gradients to 32-bit integer at workers by a scaling factor 
- However, integer aggregation might lead to the overflow at switch 


##### References
[1] https://www.usenix.org/system/files/nsdi21-lao.pdf
