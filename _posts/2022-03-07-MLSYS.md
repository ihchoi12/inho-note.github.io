---
title: "GPU overview"
date: 2022-03-07
categories:
  - System
tags:
  - GPU
---


# WIP
- ByteScheduler runs ../../tools/launch.py in the benchmark.py
- ../../tools/launch.py is not in the basic repository, so should be in the docker container
- nvidia-docker seems to be used eventually (so wanna do it on the xgpg server, instead of virsh)
- original docker is a package in ubuntu repo hosted by Canonical (https://askubuntu.com/questions/431780/apt-get-install-vs-pip-install)
- trying with docker-py: python library for docker API  (https://pypi.org/project/docker/#:~:text=SDK%20for%20Python-,A%20Python%20library%20for%20the%20Docker%20Engine%20API.,containers%2C%20manage%20Swarms%2C%20etc.)
- actullay docker-py is just an API to the system docker from python code, so anyway the system docker is needed
- move to virsh..
- virsh is inconvenient.. try docker on a node?


# ToDo
- install docker with conda (https://anaconda.org/conda-forge/docker)
- try bytescheduler with docker
- Installing nvidia-docker on Amazon EC2 (https://github.com/gngdb/nvidia-docker-ec2)
- https://towardsdatascience.com/hands-on-tensorflow-tutorial-train-resnet-50-from-scratch-using-the-imagenet-dataset-850aa31a39c0
- compare TF, PyTorch, MXNet...
- solve: $ sudo docker logs 222e9141a460 =>  Error response from daemon: configured logging driver does not support reading

# Questions
- 

# ByteScheduler

## Tensorflow Benchmark
- https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks
- run benchmark with "tf.load_library('./libplugin.so')" (as explained in the bytescheduler README)


## build docker images (3 types)
- see /byteps/bytescheduler/docker/build-docker-images.sh
- try one by one (by comment out others)
### bsc-pytorch-horovod
- docker build -t bsc-pytorch-horovod:$(date +%Y%m%d-%H%M%S) --no-cache -f pytorch_horovod.Dockerfile .
- clone ByteScheduler repo ($ git clone --branch bytescheduler --recursive https://github.com/bytedance/byteps.git)
- move to byteps/bytescheduler/docker
- $ sudo sh build-docker-images.sh
- $ sudo docker run -it -d "bsc-pytorch-horovod image ID"
- $ sudo docker exec -it bsc_CONT_ID /bin/bash
- $ python pytorch_horovod_benchmark.py --num-iters 100 --model resnet50 --num-classes 10000

### docker build -t bsc-mxnet-ps:$(date +%Y%m%d-%H%M%S) --no-cache -f mxnet_ps.Dockerfile .
- ERROR: This script does not work on Python 2.7 The minimum supported Python version is 3.7. Please use https://bootstrap.pypa.io/pip/2.7/get-pip.py instead.
- its due to the update of get-pip.py
- this Dockerfile is based on nvidia/cuda:9.0-cudnn7-devel
- the base docker image is based on centos6
- centos6 default python version: 2.6.6
- solution:
  * 1. use https://bootstrap.pypa.io/pip/2.7/get-pip.py instead (maybe it's intended by the ByteScheduler authors)
- fatal: unable to access 'https://github.com/apache/incubator-mxnet.git/': gnutls_handshake() failed: Error in the push function.
- solution: $ git config --global http.sslVerify false
- $ sudo docker run -it -d "mxnet-ps-image ID"
- $ sudo docker exec -it mxnet_ps_CONT_ID /bin/bash
- $ ./run.sh 
- OSError: libcuda.so.1: cannot open shared object file: No such file or directory
- https://askubuntu.com/a/1112008
- can I run nvidia-docker without GPU?
- maybe cannot.. (https://stackoverflow.com/questions/52030952/can-nvidia-docker-be-run-without-a-gpu)
- cuda, cuda toolkit, cuda driver...???




# GPU
- originally designed for 3D games (to support graphics consist of more polygons or pixels)
- but recently basically doing everything (supporting intensive computation power)

### Early Architecture (in 2010s)
- Handle multiple tasks: ex. vertex processing, rasterization, pixel processing, etc.
- Task Parallelism: independent tasks in parallel using pipeline
- Data Parallelism: each task in parallel with independent data 
- Configurable but not very programmable

### More programmability
- Gradually more demand on (user-specified) customized processing
- still have issue of **load-balancing**:
  * pipeline design => overall performance is bottlenecked by the slowest stage
  * if we use a complex processing in a stage, overall performance is restricted by that
- Shader Model 4.0 (to deal with load-balancing issue): unified instruction set for all stages (modern GPU architecture)
  
  
### CUDA: Compute Unified Device Architecture
- Interface between general programs and GPU
- Nvidia's SW framework for GPGPU (including libraries, compiler, API)
- Supports many languages (C, C++, Fortran, OpenCL, ...)
- To express parallel general-purpose computations
- Two components:
  * Host program: sequential threads on CPU
  * Kernels (not the OS kernels): lightweight parallel threads on GPU

##### CUDA Kernel Thread
- is a single thread running sequentially (conceptually same as the x86 CPU thread, but weaker)
- each thread has a thread-local memory (not accessible by other threads)
- Warp: a group of 32 threads with the same instructions

##### Hierarchical Thread model
- A thread (or warp)
- Thread block (consist of 512 or 1024 threads): synchronize via barriers, communicate via per-block shared memory
- Grid: consist of multiple thread blocks, communicate via per-application global memory, different kernel grids synchronize using global barriers
* what is the 'barrier'? It's a synchronization point where all threads are temporarily blocked until all of them are synchronized



### Modern GPU Architecture
- Massive data parallel architecture with unified intructions (not task-parallel pipeline anymore)
- start of GPGPU era, each of many cores (acts like a weak CPU core) can perform different instrucutions on different data (flexibility)
- has its own on-die DRAM with **high memory bandwidth**
Components:
  - DRAM interface: GPU cores access the on-die DRAM through this
  - Host interface: PCIe interface to the host CPU or other PCIe devices  
  - 16 streaming multiprocessors (SMs) run thread blocks
  - Each SM has 32 CUDA cores
  - Global L2 Cache (Shared by all SMs)
  - GigaThread block: schedule threads blocks to SMs

##### Streaming Processr
- 32 CUDA cores
- 16 load/store units
- 4 special function units
- 64KB shard memory and L1 cache
- Can run up to 1536 concurrent threads (since the memory BW is high, good to run many concurrent threads to hide memory access latency)
- Schedule in the warp granularity 

###### CUDA core and SFU
- 1 CUDA core runs 1 thread at a particular time
- CUDA core has floating point unit and interget unit
- SFU (special function unit) proivdes fast approximation for 1/x, square root, sin, cos, exp, logarithm, etc.
###### Load/Store Unit
- perform load, store and atomic memory operations
- it can access three different memory regions
  * per-thread local
  * per-block shared
  * global memory

##### CUDA Toolkit
- a development environemt for GPU accelerated applications
- it requires Nvidia GPU on the machine

### Lifetime of a GPU program
- transfer data from host m
Replicate View Server using Primary-Backup? Same issue again..emory to GPU memory (via host interface i.e., PCIe)
- host program instructs GPU to launch kernel 
- parallel execution on GPU
- transfer result from GPU memory to host memory (via host interface again)
==> overall, long latency process

*Summary: GPU is highly optimized for throughput, but not latency*

# GPU Programming
- API: from configurable (fixed) graphics processing to general purpose
- OpenGL: programmable shader, but still a graphichs API
- Now, GPGPU (CUDA, OpenCL)
  * Fully programmable
  * Dynamic control flow
  * read/write to local and global memory


### Good performance requires good understanding of the GPU HW
- Branches are not free
- minimize communication and synchronization between threads
- GPU memory hierarchy
- CPU-GPU co-processing (how to divide tasks, when to perform each task




# Python
- module: a .py file
- package: a collection of modules (in a directory)
  * we install and use packages on demand 
  * (usually) has __init__.py inside
  * a package can have sub-packages inside
- __inin__.py: notify that the current folder is a package
- import: to import a module or a function, can specify packge of the module (e.g., import package_name.module_name)
- from: to specify the package from which it imports a module
- pip: python integrated package-manaer


# Problem of python
- we have multiple projects in python
- all packges are installed globally 
- in may cause some problems or inconvenience (i.e., non-isolated env)
- we wanna use particular packages for each project (e.g., matplotlib for data analysis, PyTorch for ML)
- if those projects are totally independent, good to have virtual environment in python

# Anaconda
- it's a python distribution
- it install conda together 

### Conda
- a package manager and virtual environment manager at the same time
- specially for data science (it comes with lots of data analysis packages together)
- Conda has channels (shared by all environments) from which it looks for the packages to download (we can specify the channel using -c option e.g., conda install -c pytorch pytorch)
- To see channel list: conda config --show channels
- To add a channle into the list: conda config --add channels 'channel_name'

### Commands
- source ~/.bashrc // to initialize conda
- conda env list
- conda activate my_env
- conda deactivate



# Pytorch
- A popular DL framework
- Imperative Programming Model (not Symbolic) 
- Dynamic graph model (not static graph)
- Everything is based on Tensor (array with multiple dimension i.e., rank) operations


# Horovod
- DDL training platform using Ring-AllReduce technique
- Goal: separation of concerns between DT environment (i.e., servers, networking, containers, HW) and DL model itself
- You can train your model with scaling up to hundreds of GPUs
- Just with a few lines of python code in your existing training script
- Works for TF, PyTorch, Keras, ...
- Use bandwidth-optimal protocols like RDMA

# AWS

### What is AWS?
- a global cloud platform which allows hosting or managing services on the Internet
- Infrastructure as a Service: provides servers as a service (we don't need to manage the backup, power supply, etc.)
- Platform as a service: provides Java, Ruby, php, etc. as a service
- Software as a service: provides email services (e.g., SES), queuing services (e.g., SQS) 
- Cloud Storage platform: provides storage options like EBS, S3, etc.

### Why its popular?
- clear billing (e.g., micro billing like per-hour or per GB)
- easy sign-up 
- simple billing
- stablility (trusted vendor)

### Services
- EC2: provide bare servers
- VPC: provide a control over a part of AWS cloud, allow you create networks in the VPC and run servers on the networks
- S3: allow you to upload and share files (i.e., file storage and sharing services)
- RDS: allow you to run and manage DBs on the cloud by providing SQL, Oracle, MySQL, Aurora servers
- Route 53: DNS service
- ELB: allow you to load-balance incoming traffic to multiple machines (to scale your apps to any number of users)
- Autoscaling: add capacity on-the-fly to ELB (so that your service is never going down due to high load) 

### EC2
- provide compute capacity in the cloud
- easy to scale up/down on demand
- can be integrated into other services

##### Steps to use
- choose (or create) an AMI: software and application packages
  * AMI is a template to create a new instance based on user requirements
  * AMI includes SW, OS, applications information
  * 2 types of AMI: predefined (provided by Amazon, can be modified by user), custom (created and re-used by user)
  * AMI marketplace has lots of options
- choose instance type (which HW to use)
  * 5 main categories: 1) compute optimized, 2) memory optimized (good for in-memory cache), 3) GPU optimized, 4) storage optimized, 5) general purpose 
- configure the instance
  * number of instances
  * purchasing option (billing option)
  * network, subnet, public IP assignment
  * authentication (IAM role)
  * shutdown behavior: (stop: temporarily shut down the instance) or (terminate: return control back to Amazon)
  * in 'advanced details', user can add bootstrap scripts (that are execute when the instance is booting up)
- add additional storage to the instance
  * storage options: 1) Ephermeral, 2) EBS, 3) S3
  * user decides size, volume type, location to mount, whether encrypted
- add tags (to easily identify instances)
- configure security group (e.g., firewall to block connections based on IP addr and port num)
- review all configurations so far
- create a key pair (private, public), user keeps the private key (.pem file), Amazon keeps the pulbic key: for future authentication 


##### References
\[1\] Conda: https://www.youtube.com/watch?v=23aQdrS58e0 
\[2\] EC2: https://www.youtube.com/watch?v=8TlukLu11Yo
